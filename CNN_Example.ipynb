{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet imports for logging\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# All the necessary torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/aguerra/example-cnn-logging/d7a26ceb53ce4fc8a3376b8df62ad86b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the Comet Experiment\n",
    "experiment = Experiment(api_key=\"tHDbEydFQGW7F1MWmIKlEvrly\", project_name=\"example-cnn-logging\", workspace=\"aguerra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device so that we can easily utilize the GPU power\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'    #CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Here is our data pre-processing pipeline, made easy with Pytorch transforms\n",
    "transform_train = transforms.Compose([\n",
    "    # The below two transforms are a form of data augmentation\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# How many images we will take in every iteration, known as the batch_size\n",
    "batch_size = 128\n",
    "\n",
    "# Load the training dataset, and put it into a dataloader so it can be managed efficiently\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load the test dataset, and put it into a dataloader so it can be managed efficiently\n",
    "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# These are the potential labels we can assign our images\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Declaring all the layers of the network\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(25 * 32, 128)\n",
    "        \n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    # This is the function of the nn.Module class which tells the model what to do with an input\n",
    "    def forward(self, x):\n",
    "        # Start by sending it through the convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        # Now introduce the nonlinearity\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Repeat for second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "    \n",
    "        # Now we flatten our input so that we can use fully connnected layers\n",
    "        x = x.view(-1, 25 * 32)\n",
    "        \n",
    "        # Now use dropout with fully connected layers\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        # Return the log_softmax of the output to get the probabilities for each class\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a randomly initialized model\n",
    "model = Net().to(device)\n",
    "\n",
    "# Use negative log-likelihood loss when using a log-softmax for the final activation\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Validation Loss: 0.9385, Validation Accuracy: 45.24%\n",
      "Epoch [2/25], Validation Loss: 0.8181, Validation Accuracy: 53.31%\n",
      "Epoch [3/25], Validation Loss: 0.7994, Validation Accuracy: 52.92%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    avg_train_loss = 0\n",
    "    # For every batch of the training data\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "\n",
    "        images = images.to(device) #Transfer images to GPU\n",
    "        labels = labels.to(device) #Transfer labels to GPU\n",
    "        \n",
    "        # Extra step so pytorch understands what kind of tensors the images/labels are\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Zero out the old gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the new gradient based on loss\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "         # Track the accuracy\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Add to the total loss for the epoch, normalize by batch size\n",
    "        avg_train_loss += loss.item() / batch_size\n",
    "    \n",
    "    # After one training epoch, compute the validation statistics\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    avg_val_loss = 0\n",
    "    for images_val, labels_val in valloader:\n",
    "        images_val = images_val.to(device)\n",
    "        labels_val = labels_val.to(device)\n",
    "\n",
    "        images_val = Variable(images_val)\n",
    "        labels_val = Variable(labels_val)\n",
    "\n",
    "        outputs_val = model(images_val)\n",
    "        _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "        \n",
    "        avg_val_loss += criterion(outputs_val.detach(), labels_val).item() / batch_size\n",
    "        \n",
    "        total_val += labels_val.size(0)\n",
    "        correct_val += (predicted_val == labels_val).sum().item()\n",
    "    \n",
    "    \n",
    "    # Now, we log the training statistics to Comet\n",
    "    experiment.log_metric(\"Train Loss\", avg_train_loss, step=epoch)\n",
    "    experiment.log_metric(\"Train Accuracy\", (correct / total) * 100, step=epoch)\n",
    "    experiment.log_metric(\"Validation Loss\", avg_val_loss, step=epoch)\n",
    "    experiment.log_metric(\"Validation Accuracy\", (correct_val / total_val) * 100, step=epoch)\n",
    "    \n",
    "    # We can also save the model after every epoch, useful for early stopping!\n",
    "    tmp_file = torch.save(model.state_dict(), \"tmp/temp.model\")\n",
    "    experiment.log_asset(\"tmp/temp.model\", file_name=\"model_\" + str(epoch) + \".model\")\n",
    "    \n",
    "    # Can also print to the console \n",
    "    print('Epoch [{}/{}], Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, avg_val_loss,\n",
    "                          (correct_val / total_val) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end the experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
